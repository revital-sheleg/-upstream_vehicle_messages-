# upstream_vehicle_messages

## Getting Started

**Prerequisites**: Install dependencies before running
```bash
pip install -r requirements.txt
```

**Entry Point**: Run the main application
```bash
python generic_data_lake_new.py
```
## Project Structure

```
upstream_vehicle_messages/
â”œâ”€â”€ ğŸ“ config/
â”‚   â””â”€â”€ config_data_lake.json          # Data lake configuration, tables, partitions etc'
â”œâ”€â”€ ğŸ“ data_lake/                      # Data lake storage layers
â”‚   â”œâ”€â”€ ğŸ“ bronze/                     # Raw ingested data
â”‚   â”‚   â””â”€â”€ [table_name]/
â”‚   â”‚       â””â”€â”€ data.parquet
â”‚   â”œâ”€â”€ ğŸ“ silver/                     # Cleaned & transformed data
â”‚   â”‚   â””â”€â”€ [table_name]/
â”‚   â”‚       â””â”€â”€ data.parquet
â”‚   â””â”€â”€ ğŸ“ gold/                       # Aggregated analytics tables
â”‚       â””â”€â”€ [table_name]/
â”‚           â””â”€â”€ [aggregation_name\report].parquet
â”œâ”€â”€ ğŸ“ reporting/                      # Business reports output
â”‚   â””â”€â”€ [table_name]/
â”‚       â”œâ”€â”€ report1.csv
â”‚       â””â”€â”€ report2.json
â”œâ”€â”€ ğŸ“„ generic_data_lake.py        # Main application entry point
â”œâ”€â”€ ğŸ“„ generic_data_processor.py       # Core data processing engine
â”œâ”€â”€ ğŸ“„ config_manager.py               # Configuration management
â”œâ”€â”€ ğŸ“„ mock_data_generator.py          # Mock data generation
â”œâ”€â”€ ğŸ“„ requirements.txt                # Python dependencies
â””â”€â”€ ğŸ“„ README.md                       # Project documentation
```

### Key Components

- **Main Application**: `generic_data_lake.py` - Interactive menu-driven interface
- **Processing Engine**: `generic_data_processor.py` - Configurable data processing logic
- **Configuration**: `config_data_lake.json` - Table definitions, layer configs, and processing rules
- **Data Storage**: Organized in bronze/silver/gold layers with Parquet format
- **Reports**: Generated in `reporting/` directory in CSV/JSON formats



## Menu Options

The data lake application provides the following main operations:

### 1. Run Data Pipeline
Complete data processing pipeline with multiple sub-options:

**Data Source Options:**
- API (requires Docker container)
- Mock data

**Table Selection:**
- Process all tables (currently only one table available, but configuration suppports additional tables)
- Select specific tables

**Processing Mode:**
- Full pipeline (bronze â†’ silver â†’ gold â†’ reporting)
- Single layer processing:
  - Bronze layer only
  - Silver layer only (assume data in earlier stages)
  - Gold layer only (assume data in earlier stages)
  - Reporting layer only (assume data in earlier stages)

### 2. Run SQL Injection Violation Report
**data is needed in bronze layer
Security scanning functionality with options for:

**Table Selection:**
- 1.Scan all tables (currently only one table available, but configuration suppports additional tables)
- 2. Select specific tables

**Column Selection:**
- Use default columns (vin)
- Specify custom columns

**Pattern Selection:**
- Use default SQL injection patterns
- Specify custom regex patterns

## Data Lake Architecture

The table-based data lake follows a layered architecture:

- **Bronze Layer**: Raw ingested data in original format
- **Silver Layer**: Cleaned and transformed data with applied business rules
- **Gold Layer**: Aggregated tables optimized for analytics. Can hold longer period\ scope than report **Consider Retention here
- **Reporting**: Business reports in CSV/JSON format (including security report)

## Considerations and Future Improvements 

**Spark Integration**: This project currently uses pandas for simplicity. For production scale, consider migrating to Apache Spark for distributed processing.

**Data Lake Formats**: Open table formats like Apache Iceberg are supported (partial implementation). Consider full migration for:


